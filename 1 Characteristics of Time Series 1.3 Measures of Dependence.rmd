---
title: "1 Characteristics of Time Series 1.3 Measures of Dependence"
author: "Aaron Smith"
date: '2022-10-23'
output: html_document
---

This code is modified from Time Series Analysis and Its Applications, by Robert H. Shumway, David S. Stoffer 
https://github.com/nickpoison/tsa4

UCF students can download it for free through the library.

```{r,eval = FALSE}
#install.packages(
#  pkgs = "remotes"
#)
#remotes::install_github(
#  repo = "nickpoison/astsa/astsa_build"
#)
```

```{r}
options(
  digits = 3,
  scipen = 99
)
rm(
  list = ls()
)
```


The multidimensional cumulative distribution function gives a complete representation of a time series for $n$ points of time, $t_1,t_2,...,t_n$. Unfortunately, this function is usually unwieldy and cannot be written is a useful manner. Furthermore, creating informative visualizations is virtually impossible.

$$
F_{t_1.t_2,...,t_n}\left(c_1,c_2,\ldots,c_n\right) = P\left(x_{t_1} \leq c_1,x_{t_2} \leq c_2,\ldots,x_{t_n} \leq c_n\right)
$$

We can gain insights into our time series by investigating one time point at a time by using the marginal cumulative distribution function, and the marginal density function.

$$
F_t(x) = P(x_t \leq x) \\
f_t(x) = \dfrac{\partial F_t(x)}{\partial x}
$$

Example: If $x_t$ is Gaussian with mean $\mu_t$ and variance $\sigma_t^2$, abbreviated $x_t \sim Normal(\mu_t,\sigma_t^2)$, then

$$
f_t(x) = \dfrac{1}{\sigma_t \sqrt{2\pi}}exp\left(\dfrac{-\left(x - \mu_t\right)^2}{2\sigma_t^2} \right)
$$

# Definition 1.1 The mean function

The mean function is defined as

$$
\mu_{xt} = E(x_t) = \int_{-\infty}^{\infty} x f_t(x) dx
$$

When the expected value exists.

Notice that the expected value of $X$ is a function on $t$.

# Example 1.13 Mean function of a moving average series

$$
w_t \text{ is a white noise series} \\
\mu_{w_t} = E(w_t) = 0
$$ 

Consider the smoothing series:

$$
v_t = \dfrac{w_{t-1} + w_{t} + w_{t+1}}{3} \\
E(v_t) = \dfrac{E(w_{t-1}) + E(w_{t}) + E(w_{t+1})}{3} = 0
$$

# Example 1.14 mean function of a random walk with drift

$$
x_t = \delta t + \sum_{j=1}^{t} w_j, \ t \in \mathbb{N} \\
E(w_t) = 0 \forall t \\
\delta \text{ is constant}
$$
$$
\mu_{x_t} = E(x_t) = \delta t + \sum_{j=1}^{t} E(w_j) = \delta t
$$

# Example 1.15 mean function of signal plus noise

For a model of a fixed signal waveform plus zero-mean white noise, the expected value is the waveform.

$$
x_t = 2\cos\left( 2 \pi \dfrac{t+15}{50} \right) + w_t \\
E(x_t) = E\left(2\cos\left( 2 \pi \dfrac{t+15}{50} \right) + w_t\right) \\
E(x_t) = 2\cos\left( 2 \pi \dfrac{t+15}{50} \right) + E\left(w_t\right) \\
E(x_t) = 2\cos\left( 2 \pi \dfrac{t+15}{50} \right) \\
$$

# Definition 1.2 autocovariance function

One way that we can assess the lack of independence between two points in a series is the autocovariance function

$$
\gamma_x(s,t) = cov(x_s,x_t) = E\left[(x_s - \mu_s)(x_t - \mu_t)\right]
$$

When variance is finite, we can extend this to autocorrelation.

Autocovariance measures the linear dependence between two points on the same series observed at different times.

* very smooth series exhibit autocovariance functions that stay large even when the $t$ and $s$ are far apart
* choppy series tend to have autocovariance functions that are nearly zero for large separations.


Recall from classical statistics that if 

$$
\gamma_x(s,t) = 0
$$

then $x_s$ and $x_t$ are not linearly related, but there still may be some dependence structure between them.

# Example showing that zero correlation does not mean independent

```{r}
t <- seq(
  from = 0,
  to = 2*pi,
  by = pi/12
)
x <- cos(t)
y <- sin(t)
cov(x,y)
cor(x,y)
plot(
  x = x,
  y = y,
  asp = 1
)
```

# Special situation: When normal, zero correlation = independence

If $x_s,x_t$ are bivariate normal then zero correlation means that $x_s$ and $x_t$ are independent.

# Special case: $s$ = $t$

$$
\gamma_x(t,t) = E\left[(x_t - \mu_t)^2\right] = Variance(x_t)
$$

# Example 1.16 autocorrelation of white noise

If $w_t$ is a white-noise series with $E(w_t) = 0$ and

$$
\gamma_w(s,t) = cov(w_s,w_t) = \begin{cases}
0 & \text{ if } s \neq t\\
\sigma_w^2 & \text{ if } s = t
\end{cases}
$$

# Property 1.1 covariance of linear combinations

If ${X_j}$, ${Y_k}$ are random variables (any random variables), and $U,V$ are linear combination of these random variables,

$$
U = \sum_{j=1}^{m}a_jX_j \\
V = \sum_{k=1}^{r}b_k Y_k
$$

then

$$
cov(U,V) = \sum_{j = 1}^{m}\sum_{k=1}^{r} a_j b_k cov(X_j,Y_k)
$$

and 
$$
Variance(U) = cov(U,U) \\
Variance(V) = cov(V,V)
$$

# Example 1.17 Autocovariance of a Moving Average

Consider applying a three-point moving average to the white noise series wt of the previous example

$$
\begin{aligned}
\gamma_v(s,t) =& cov(v_s,v_t) \\
=& cov\left(\dfrac{w_{s-1} + w_s + w_{s+1}}{3},\dfrac{w_{t-1} + w_t + w_{t+1}}{3}\right)
\end{aligned}
$$

When $s = t$:

$$
\begin{aligned}
\gamma_v(s,t) =& cov(v_t,v_t) \\
=& cov\left(\dfrac{w_{t-1} + w_t + w_{t+1}}{3},\dfrac{w_{t-1} + w_t + w_{t+1}}{3}\right) \\
=& \sum_{j = t-1}^{t+1}\sum_{k=t-1}^{t+1} \dfrac{1}{9} cov(w_j,w_k) \\
=& \sum_{j = k} \dfrac{1}{9} cov(w_j,w_k) + \sum_{j \neq k} 0 \\
=& \dfrac{1}{9} cov(w_{t-1},w_{t-1}) + \dfrac{1}{9} cov(w_{t},w_{t}) + \dfrac{1}{9} cov(w_{t+1},w_{t+1}) \\
=& \dfrac{1}{3}\sigma_w^2
\end{aligned}
$$

When $s = t+1$

$$
\begin{aligned}
\gamma_v(s,t) =& cov(v_{t+1},v_t) \\
=& cov\left(\dfrac{w_{t} + w_{t+1} + w_{t+2}}{3},\dfrac{w_{t-1} + w_t + w_{t+1}}{3}\right) \\
=& \dfrac{1}{9}\left(cov(w_t,w_t) + cov(w_{t+1},w_{t+1})\right) \\
\dfrac{2}{9}\sigma_w^2
\end{aligned}
$$
$$
\begin{aligned}
\gamma_v(s,t) =& cov(v_s,v_t) = \begin{cases} \\
      \dfrac{1}{9}\sigma_w^2 & when \ s = t-2 \\
      \dfrac{2}{9}\sigma_w^2 & when \ s = t-1 \\
      \dfrac{3}{9}\sigma_w^2 & when \ s = t \\      
      \dfrac{2}{9}\sigma_w^2 & when \ s = t+1 \\
      \dfrac{1}{9}\sigma_w^2 & when \ s = t+2 \\
      0 & when \ |s-t| > 2 \\
   \end{cases}
\end{aligned}
$$

This shows that smoothing this time series with the moving average gives a covariance that decreases with the separation of time.

This covariance function depends only on the time separation (lag), but not the location of the points in time nor time series value. Such times series suggests weak stationarity (mean does not vary with time, autocovariance does not vary with time (location not difference), variance is finite).

# Example 1.18 Autocovariance of a random walk

For the random walk from uncorrelated random variables:

$$
\begin{aligned}
{w_t} &\text{ are uncorrelated random variables} \\
x_t =& \sum_{j=1}^{t}w_j \\
\gamma_x(s,t) =& cov\left(\sum_{j=1}^{s}w_j,\sum_{k=1}^{t}w_k\right) \\
=& cov\left(\sum_{j=1}^{min(s,t)}w_j,\sum_{k=1}^{min(s,t)}w_k + \sum_{k=min(s,t) + 1}^{max(s,t)}w_k\right) \\
=& cov\left(\sum_{j=1}^{min(s,t)}w_j,\sum_{k=1}^{min(s,t)}w_k\right) +  cov\left(\sum_{j=1}^{min(s,t)}w_j,\sum_{k=min(s,t) + 1}^{max(s,t)}w_k\right) \\
=& min(s,t)\sigma_w^2 + 0
\end{aligned}
$$

The autocovariance function of a random walk depends on the particular time values, and not on the time separation or lag.

Notice that the variance of the random walk, $var(x_t) = \gamma_x(t, t) = t \sigma_w^2$, increases without bound as time t increases.

```{r}
set.seed(
  seed = 41507
)
n_observations <- 200
n_randomwalks <- 1000
M_randomwalk <- as.data.frame(
  x = matrix(
    data = rnorm(
      n = n_observations*n_randomwalks
    ),
    nrow = n_observations,
    ncol = n_randomwalks
  )
)
M_randomwalk <- as.data.frame(sapply(
  X = M_randomwalk,
  FUN = cumsum
))
M_randomwalk$t <- 1:n_observations
M_randomwalk <- tidyr::gather(
  data = M_randomwalk,
  key = "randomwalk",
  value = "value",
  -t
)
library(ggplot2)
ggplot(M_randomwalk) + 
  aes(x = t,y = value,group = randomwalk) + 
  geom_line(alpha = 0.5,color = "black") +
  labs(
    title = "Line plot of multiple Gaussian random walks",
    subtitle = "Notice how the vertical spread of random walks increases as t increases"
  ) +
  theme_bw()
v_var <- tapply(
  X = M_randomwalk$value,
  INDEX = M_randomwalk$t,
  FUN = var
)
ggplot(data.frame(
  t = as.numeric(
    x = names(v_var)
  ),
  var = v_var
)) + 
  aes(x = t) + 
  geom_line(mapping = aes(y = t),color = "red") + 
  geom_line(mapping = aes(y = var),color = "blue") + 
  ggtitle("Line plot of sample variance from the random walks") +
  ylab("sample variance")
```

# Definition 1.3: The autocorrelation function (ACF)

$$
\rho(s,t) = \dfrac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}
$$

The Cauchy-Schwartz inequality shows that autocorrelation is between -1 and 1.

If $\rho(s,t) = \pm 1$, then $x_t = \beta_0 + \beta_1 x_s$, and the sign of the coefficient matches the sign of $\rho(s,t)$.

# Definition 1.4: The cross-covariance function

$$
\gamma_{xy}(s,t) = cov(x_s,y_t) = E\left[(x_s - \mu_{x_s})(y_t - \mu_{y_t})\right]
$$

# Definition 1.5: The cross-correlation function (CCF)

$$
\rho_{xy}(s,t) = \dfrac{E\left[(x_s - \mu_{x_s})(y_t - \mu_{y_t})\right]}{\sqrt{\gamma_x(s,s)\gamma_y(t,t)}}
$$

# Multivariate time series

Multivariate time series with $r$ components

$$
(x_{t1},x_{t2},\ldots,x_{tr})^T
$$

# Covariance of a multivariate time series

$$
\gamma_{jk} = E\left[(x_{sj} - \mu_{sj})(y_{tk} - \mu_{tk})\right] \ \forall j,k = 1,2,\ldots,r
$$

We can compute covariance or correlation along time, or component, or both.
